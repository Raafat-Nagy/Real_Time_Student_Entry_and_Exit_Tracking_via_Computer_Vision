{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50875038",
   "metadata": {},
   "source": [
    "# **Real-Time Student Entry and Exit Tracking via Computer Vision**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb448626",
   "metadata": {},
   "source": [
    "### Project Overview:\n",
    "\n",
    "This project presents an automated system that leverages **computer vision** to **detect, track, and count students** as they **enter and exit lecture halls** in real-time.  \n",
    "The system utilizes advanced object detection (YOLO) and tracking algorithms to determine the **direction of movement (IN or OUT)** based on studentsâ€™ trajectories relative to predefined virtual lines or zones.\n",
    "\n",
    "It can be integrated into university infrastructure to **automate attendance logging**, analyze **student flow**, and **enhance campus monitoring** with no human intervention.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- Real-time object detection and tracking using **YOLO**\n",
    "- Direction-aware logic to determine **entry and exit** events\n",
    "- High-speed video processing using **OpenCV** and **NumPy**\n",
    "- Region detection and zone logic using **Shapely**\n",
    "- Event logging to **CSV** or **database** for analytics\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- Smart attendance tracking\n",
    "- Lecture hall capacity monitoring\n",
    "- Crowd movement analysis in academic environments\n",
    "- Integration with university management systems\n",
    "\n",
    "---\n",
    "\n",
    "### Technologies Used:\n",
    "\n",
    "- Python\n",
    "- OpenCV\n",
    "- YOLO (You Only Look Once)\n",
    "- NumPy \n",
    "- Shapely\n",
    "- CSV / Database logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210744a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae4da6",
   "metadata": {},
   "source": [
    "## CSV Data Recorder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20134ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "\n",
    "class CSVDataRecorder:\n",
    "    \"\"\"Manage data records by storing values and timestamps in a CSV file.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, file_path: str, headers: List[str], time_format: str = \"%H:%M:%S\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the data recorder.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            headers: Column headers (excluding time)\n",
    "            time_format: Format for the time column (default: \"%H:%M:%S\")\n",
    "        \"\"\"\n",
    "        self.file_path = self._create_dated_filepath(Path(file_path))\n",
    "        self.headers = [*headers, \"Time\"]\n",
    "        self.time_format = time_format\n",
    "        self._ensure_header_exists()\n",
    "\n",
    "    def _create_dated_filepath(self, base_path: Path) -> Path:\n",
    "        \"\"\"Generate filename with current date prefix.\"\"\"\n",
    "        date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "        return base_path.parent / f\"{date_str}_{base_path.name}\"\n",
    "\n",
    "    def _ensure_header_exists(self) -> None:\n",
    "        \"\"\"Create file with headers if it doesn't exist or is empty.\"\"\"\n",
    "        self.file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.file_path.exists() or self.file_path.stat().st_size == 0:\n",
    "            with self.file_path.open(\"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow(self.headers)\n",
    "\n",
    "    def _prepare_row(self, record: Union[Dict[str, Any], List[Any]]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Validate and format record with timestamp.\n",
    "\n",
    "        Args:\n",
    "            record: Data to record (dict or list)\n",
    "\n",
    "        Returns:\n",
    "            Formatted row with timestamp\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If record format is invalid\n",
    "        \"\"\"\n",
    "        if isinstance(record, dict):\n",
    "            expected_keys = self.headers[:-1]\n",
    "            if set(record.keys()) != set(expected_keys):\n",
    "                raise ValueError(\n",
    "                    f\"Expected keys: {expected_keys}, got: {list(record.keys())}\"\n",
    "                )\n",
    "            values = [record[key] for key in expected_keys]\n",
    "        elif isinstance(record, list):\n",
    "            if len(record) != len(self.headers) - 1:\n",
    "                raise ValueError(\n",
    "                    f\"Expected {len(self.headers)-1} values, got {len(record)}\"\n",
    "                )\n",
    "            values = record\n",
    "        else:\n",
    "            raise TypeError(\"Record must be a dict or list.\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(self.time_format)\n",
    "        return [*values, timestamp]\n",
    "\n",
    "    def add_row(self, record: Union[Dict[str, Any], List[Any]]) -> None:\n",
    "        \"\"\"Add a single record to the CSV file.\"\"\"\n",
    "        row = self._prepare_row(record)\n",
    "        self._write_rows([row])\n",
    "\n",
    "    def add_rows(self, records: List[Union[Dict[str, Any], List[Any]]]) -> None:\n",
    "        \"\"\"Add multiple records to the CSV file.\"\"\"\n",
    "        rows = [self._prepare_row(r) for r in records]\n",
    "        self._write_rows(rows)\n",
    "\n",
    "    def _write_rows(self, rows: List[List[Any]]) -> None:\n",
    "        \"\"\"Write rows to CSV file.\"\"\"\n",
    "        with self.file_path.open(\"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c2490",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402860c",
   "metadata": {},
   "source": [
    "## Drawing Utilities for Visualizing Object Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe0b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def put_text_rect(\n",
    "    image,\n",
    "    text: str,\n",
    "    position,\n",
    "    scale: float = 0.7,\n",
    "    thickness: int = 1,\n",
    "    text_color=(0, 0, 0),\n",
    "    rect_color=(255, 255, 255),\n",
    "    padding: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws text with a background rectangle on the given image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image on which to draw the text and rectangle.\n",
    "        text (str): The text to display.\n",
    "        position (tuple): The (x, y) coordinates of the bottom-left corner of the text.\n",
    "        scale (float): Font scale factor.\n",
    "        thickness (int): Thickness of the text lines.\n",
    "        text_color (tuple): Color of the text in BGR format.\n",
    "        rect_color (tuple): Color of the background rectangle in BGR format.\n",
    "        padding (int): Padding around the text within the rectangle.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    x, y = position\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width, text_height), _ = cv2.getTextSize(\n",
    "        text, cv2.FONT_HERSHEY_SIMPLEX, scale, thickness\n",
    "    )\n",
    "\n",
    "    # Calculate the coordinates of the rectangle\n",
    "    rect_top_left = (x - padding, y + padding)\n",
    "    rect_bottom_right = (x + text_width + padding, y - text_height - padding)\n",
    "\n",
    "    # Draw the background rectangle\n",
    "    cv2.rectangle(image, rect_top_left, rect_bottom_right, rect_color, cv2.FILLED)\n",
    "\n",
    "    # Draw the text\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text,\n",
    "        (x, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        scale,\n",
    "        text_color,\n",
    "        thickness,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "\n",
    "def draw_box_label(\n",
    "    image,\n",
    "    box,\n",
    "    label: str | None = None,\n",
    "    scale: float = 0.7,\n",
    "    thickness: int = 1,\n",
    "    text_color=(0, 0, 0),\n",
    "    rect_color=(255, 255, 255),\n",
    "    padding=5,\n",
    "    box_color=(255, 255, 255),\n",
    "    box_thickness=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws a bounding box with a label on the given image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): The image on which to draw the box and label.\n",
    "        box (tuple): The bounding box coordinates in xyxy format (x1, y1, x2, y2).\n",
    "        label (str): The label text to display.\n",
    "        scale (float): Font scale factor.\n",
    "        thickness (int): Thickness of the text lines.\n",
    "        text_color (tuple): Color of the text in BGR format.\n",
    "        rect_color (tuple): Color of the background rectangle for the label in BGR format.\n",
    "        padding (int): Padding around the text within the rectangle.\n",
    "        box_color (tuple): Color of the bounding box in BGR format.\n",
    "        box_thickness (int): Thickness of the bounding box lines.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), box_color, box_thickness)\n",
    "\n",
    "    if label is not None:\n",
    "        # Get the size of the text\n",
    "        (text_width, text_height), _ = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, scale, thickness\n",
    "        )\n",
    "\n",
    "        # Calculate the position for the label background rectangle\n",
    "        rect_top_left = (x1, y1 - text_height - 2 * padding)\n",
    "        rect_bottom_right = (x1 + text_width + 2 * padding, y1)\n",
    "\n",
    "        # Draw the background rectangle for the label\n",
    "        cv2.rectangle(image, rect_top_left, rect_bottom_right, rect_color, cv2.FILLED)\n",
    "\n",
    "        # Draw the label text\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label.capitalize(),\n",
    "            (x1 + padding, y1 - padding),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            scale,\n",
    "            text_color,\n",
    "            thickness,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "\n",
    "def draw_centroid_and_tracks(\n",
    "    img,\n",
    "    track: list,\n",
    "    max_track: int = 20,\n",
    "    color: tuple = (255, 255, 255),\n",
    "    track_thickness: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw centroid point and track trails.\n",
    "\n",
    "    Args:\n",
    "        img (Image.Image or numpy array): The image to annotate.\n",
    "        track (list): object tracking points for trails display\n",
    "        color (tuple): tracks line color\n",
    "        track_thickness (int): track line thickness value\n",
    "    \"\"\"\n",
    "    if len(track) >= max_track:\n",
    "        track = track[(-max_track - 1) : -1]\n",
    "    points = np.array(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "    cv2.polylines(img, [points], isClosed=False, color=color, thickness=track_thickness)\n",
    "    cv2.circle(\n",
    "        img, (int(track[-1][0]), int(track[-1][1])), track_thickness * 2, color, -1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc992b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9248c2f",
   "metadata": {},
   "source": [
    "## Hall Status API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ee8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def report_hall_event(status: Literal[\"IN\", \"OUT\"], hall_id: int = 1):\n",
    "    \"\"\"\n",
    "    Reports a hall event (entry or exit) to the remote API.\n",
    "\n",
    "    Args:\n",
    "        status (Literal[\"IN\", \"OUT\"]): The event status, must be either \"IN\" or \"OUT\".\n",
    "        hall_id (int): The ID of the hall to include in the API call.\n",
    "\n",
    "    Behavior:\n",
    "        - Sends an HTTP GET request to the backend API to log entry or exit.\n",
    "        - If status is invalid, prints a warning and exits.\n",
    "        - On success, prints the API response JSON.\n",
    "        - On failure, prints the status code and error text.\n",
    "\n",
    "    Example:\n",
    "        report_hall_event(\"IN\", hall_id=3)\n",
    "    \"\"\"\n",
    "    if status == \"IN\":\n",
    "        url = f\"https://nextgenedu-database.azurewebsites.net/api/hall/enter/{hall_id}\"\n",
    "    elif status == \"OUT\":\n",
    "        url = f\"https://nextgenedu-database.azurewebsites.net/api/hall/exit/{hall_id}\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid status. Use 'IN' or 'OUT'.\")\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Success:\", response.json())\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90931ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46b960",
   "metadata": {},
   "source": [
    "## Object Counting and Tracking Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebfc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Object Counting and Tracking Module\n",
    "\n",
    "This module provides functionality for counting objects crossing defined regions in video streams.\n",
    "It supports both line and polygon counting regions with directional counting (IN/OUT) capabilities.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "# from ..utils import draw, CSVDataRecorder\n",
    "# from ..api.hall_status_api import report_hall_event\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    \"\"\"Enumeration for the direction of object movement.\n",
    "\n",
    "    Attributes:\n",
    "        IN: Represents movement into the defined area\n",
    "        OUT: Represents movement out of the defined area\n",
    "    \"\"\"\n",
    "\n",
    "    IN = \"IN\"\n",
    "    OUT = \"OUT\"\n",
    "\n",
    "\n",
    "class RegionType(Enum):\n",
    "    \"\"\"Enumeration for the type of counting region.\n",
    "\n",
    "    Attributes:\n",
    "        LINE: Straight line counting region\n",
    "        POLYGON: Polygon-shaped counting region\n",
    "    \"\"\"\n",
    "\n",
    "    LINE = \"LINE\"\n",
    "    POLYGON = \"POLYGON\"\n",
    "\n",
    "\n",
    "class Orientation(Enum):\n",
    "    \"\"\"Enumeration for the orientation of the counting region.\n",
    "\n",
    "    Attributes:\n",
    "        HORIZONTAL: Horizontally oriented region\n",
    "        VERTICAL: Vertically oriented region\n",
    "    \"\"\"\n",
    "\n",
    "    HORIZONTAL = \"HORIZONTAL\"\n",
    "    VERTICAL = \"VERTICAL\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegionInfo:\n",
    "    \"\"\"Container for counting region metadata and geometry.\n",
    "\n",
    "    Attributes:\n",
    "        region_type: Type of region (LINE or POLYGON)\n",
    "        geometry: Shapely geometry object representing the region\n",
    "        orientation: Primary orientation of the region (HORIZONTAL or VERTICAL)\n",
    "    \"\"\"\n",
    "\n",
    "    region_type: RegionType\n",
    "    geometry: LineString | Polygon\n",
    "    orientation: Orientation\n",
    "\n",
    "\n",
    "class ObjectCounter:\n",
    "    \"\"\"\n",
    "    Main class for object counting and tracking across defined regions.\n",
    "\n",
    "    Features:\n",
    "    - Tracks object movement across lines or polygons\n",
    "    - Counts IN/OUT directions based on region orientation\n",
    "    - Maintains class-specific counts\n",
    "    - Visualizes counting regions and object tracks\n",
    "    - Optional CSV logging of counting events\n",
    "\n",
    "    Usage Example:\n",
    "        >>> counter = ObjectCounter(\n",
    "        ...     region=[(100, 200), (300, 200)],\n",
    "        ...     class_names={0: 'person', 1: 'car'},\n",
    "        ...     csv_path='counts.csv'\n",
    "        ... )\n",
    "        >>> while processing_frames:\n",
    "        ...     processed_frame = counter.process_frame(frame, boxes, track_ids, class_ids)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        region: List[Tuple[int, int]],\n",
    "        class_names: Dict[int, str],\n",
    "        csv_path: str | None = None,\n",
    "        draw_boxes: bool = True,\n",
    "        draw_tracking: bool = True,\n",
    "        show_labels: bool = True,\n",
    "        region_color: Tuple[int, int, int] = (200, 200, 0),\n",
    "        line_thickness: int = 2,\n",
    "        show_in_count: bool = True,\n",
    "        show_out_count: bool = True,\n",
    "        track_history_len: int = 25,\n",
    "        crossing_threshold: float = 0.5,\n",
    "        send_api_events: bool = True,\n",
    "        hall_id: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initializes the ObjectCounter with configuration parameters.\n",
    "\n",
    "        Args:\n",
    "            region: List of (x,y) points defining counting region (min 2 points)\n",
    "            class_names: Dictionary mapping class IDs to display names\n",
    "            csv_path: Optional path for CSV output of counting events\n",
    "            draw_boxes: Toggles bounding box drawing\n",
    "            draw_tracking: Toggles movement trail drawing\n",
    "            show_labels: Toggles display of object labels\n",
    "            region_color: BGR color for region visualization\n",
    "            line_thickness: Pixel width for drawn elements\n",
    "            show_in_count: Toggles display of IN counts\n",
    "            show_out_count: Toggles display of OUT counts\n",
    "            track_history_len: Number of positions to retain for trail visualization\n",
    "            crossing_threshold: Minimum movement distance to check for crossing\n",
    "        \"\"\"\n",
    "        self._validate_inputs(region, class_names)\n",
    "\n",
    "        self.region = np.array(region, dtype=np.int32)\n",
    "        self.class_names = class_names\n",
    "        self.csv_path = csv_path\n",
    "        self.draw_tracking = draw_tracking\n",
    "        self.draw_boxes = draw_boxes\n",
    "        self.show_labels = show_labels\n",
    "        self.region_color = region_color\n",
    "        self.line_thickness = line_thickness\n",
    "        self.show_in_count = show_in_count\n",
    "        self.show_out_count = show_out_count\n",
    "        self.track_history_len = max(2, track_history_len)\n",
    "        self.crossing_threshold = crossing_threshold\n",
    "\n",
    "        self.send_api_events = send_api_events\n",
    "        self.hall_id = hall_id\n",
    "\n",
    "        self.region_info = self._compute_region_info()\n",
    "        display(__class__.__name__, vars(self))\n",
    "\n",
    "        self.in_count_total = 0\n",
    "        self.out_count_total = 0\n",
    "        self.counted_ids: Set[int] = set()\n",
    "        self.class_direction_counts = defaultdict(\n",
    "            lambda: {Direction.IN.value: 0, Direction.OUT.value: 0}\n",
    "        )\n",
    "        self.object_histories = defaultdict(list)\n",
    "\n",
    "        self.csv_logger = self._initialize_csv_logger() if self.csv_path else None\n",
    "        self.csv_row_id = 1\n",
    "\n",
    "    def _validate_inputs(\n",
    "        self, points: List[Tuple[int, int]], names: Dict[int, str]\n",
    "    ) -> None:\n",
    "        \"\"\"Validates constructor inputs meet minimum requirements.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If inputs fail validation checks\n",
    "        \"\"\"\n",
    "        if len(points) < 2:\n",
    "            raise ValueError(\"The 'region' list must contain at least 2 points.\")\n",
    "        if not names:\n",
    "            raise ValueError(\"The 'class_names' dictionary cannot be empty.\")\n",
    "        for i, point in enumerate(points):\n",
    "            if not isinstance(point, (tuple, list)) or len(point) != 2:\n",
    "                raise ValueError(\n",
    "                    f\"Point {i} in region must be a tuple/list of 2 coordinates.\"\n",
    "                )\n",
    "\n",
    "    def _initialize_csv_logger(self) -> CSVDataRecorder:\n",
    "        \"\"\"Configures CSV logging with standard headers.\n",
    "\n",
    "        Returns:\n",
    "            Configured CSVDataRecorder instance\n",
    "        \"\"\"\n",
    "        headers = [\"ID\", \"ClassName\", \"Direction\", \"Timestamp\"]\n",
    "        return CSVDataRecorder(self.csv_path, headers)\n",
    "\n",
    "    def _compute_region_info(self) -> RegionInfo:\n",
    "        \"\"\"Analyzes region geometry to determine type and orientation.\n",
    "\n",
    "        Returns:\n",
    "            RegionInfo: Contains processed region characteristics\n",
    "        \"\"\"\n",
    "        num_points = len(self.region)\n",
    "\n",
    "        if num_points == 2:\n",
    "            region_type = RegionType.LINE\n",
    "            shape_geometry = LineString(self.region)\n",
    "            width = abs(self.region[0][0] - self.region[1][0])\n",
    "            height = abs(self.region[0][1] - self.region[1][1])\n",
    "        else:\n",
    "            region_type = RegionType.POLYGON\n",
    "            shape_geometry = Polygon(self.region)\n",
    "            x_coords, y_coords = self.region[:, 0], self.region[:, 1]\n",
    "            width = x_coords.max() - x_coords.min()\n",
    "            height = y_coords.max() - y_coords.min()\n",
    "\n",
    "        orientation = Orientation.HORIZONTAL if width > height else Orientation.VERTICAL\n",
    "        return RegionInfo(\n",
    "            region_type=region_type, geometry=shape_geometry, orientation=orientation\n",
    "        )\n",
    "\n",
    "    def draw_region(self, image: np.ndarray) -> None:\n",
    "        \"\"\"Renders the counting region on the provided image.\n",
    "\n",
    "        Args:\n",
    "            image: The numpy array (frame) to draw on\n",
    "        \"\"\"\n",
    "        is_polygon = self.region_info.region_type == RegionType.POLYGON\n",
    "        cv2.polylines(\n",
    "            image,\n",
    "            [self.region],\n",
    "            is_polygon,\n",
    "            self.region_color,\n",
    "            self.line_thickness,\n",
    "        )\n",
    "\n",
    "        for point in self.region:\n",
    "            cv2.circle(image, tuple(point), 7, self.region_color, -1)\n",
    "            cv2.circle(image, tuple(point), 8, (255, 255, 255), 1)\n",
    "\n",
    "    def _is_crossing_region(\n",
    "        self, current_pos: Tuple[int, int], prev_pos: Tuple[int, int]\n",
    "    ) -> bool:\n",
    "        \"\"\"Determines if movement between positions crosses the region.\n",
    "\n",
    "        Args:\n",
    "            current_pos: (x,y) of current object position\n",
    "            prev_pos: (x,y) of previous object position\n",
    "\n",
    "        Returns:\n",
    "            bool: True if crossing detected, False otherwise\n",
    "        \"\"\"\n",
    "        if current_pos is None or prev_pos is None:\n",
    "            return False\n",
    "\n",
    "        movement_distance = np.linalg.norm(np.array(current_pos) - np.array(prev_pos))\n",
    "        if movement_distance < self.crossing_threshold:\n",
    "            return False\n",
    "\n",
    "        if self.region_info.region_type == RegionType.LINE:\n",
    "            movement_line = LineString([prev_pos, current_pos])\n",
    "            return self.region_info.geometry.intersects(movement_line)\n",
    "        else:\n",
    "            polygon = self.region_info.geometry\n",
    "            is_current_inside = polygon.contains(Point(current_pos))\n",
    "            is_prev_inside = polygon.contains(Point(prev_pos))\n",
    "            return is_current_inside != is_prev_inside\n",
    "\n",
    "    def _determine_direction(\n",
    "        self, current_pos: Tuple[int, int], prev_pos: Tuple[int, int]\n",
    "    ) -> Direction:\n",
    "        \"\"\"Calculates movement direction relative to region orientation.\n",
    "\n",
    "        Args:\n",
    "            current_pos: Current object position\n",
    "            prev_pos: Previous object position\n",
    "\n",
    "        Returns:\n",
    "            Direction: IN or OUT based on movement analysis\n",
    "        \"\"\"\n",
    "        if self.region_info.orientation == Orientation.VERTICAL:\n",
    "            return Direction.IN if current_pos[0] > prev_pos[0] else Direction.OUT\n",
    "        else:\n",
    "            return Direction.IN if current_pos[1] > prev_pos[1] else Direction.OUT\n",
    "\n",
    "    def _process_object_count(\n",
    "        self,\n",
    "        current_pos: Tuple[int, int],\n",
    "        prev_pos: Tuple[int, int] | None,\n",
    "        track_id: int,\n",
    "        class_id: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Handles counting logic for individual object movements.\n",
    "\n",
    "        Args:\n",
    "            current_pos: Current object position\n",
    "            prev_pos: Previous object position (None if first detection)\n",
    "            track_id: Unique identifier for the object\n",
    "            class_id: Class identifier for the object\n",
    "        \"\"\"\n",
    "        if prev_pos is None or track_id in self.counted_ids:\n",
    "            return\n",
    "\n",
    "        is_crossing = self._is_crossing_region(current_pos, prev_pos)\n",
    "\n",
    "        if is_crossing:\n",
    "            direction = self._determine_direction(current_pos, prev_pos)\n",
    "\n",
    "            class_name = self.class_names[class_id]\n",
    "            if direction == Direction.IN:\n",
    "                self.in_count_total += 1\n",
    "                self.class_direction_counts[class_name][Direction.IN.value] += 1\n",
    "            else:\n",
    "                self.out_count_total += 1\n",
    "                self.class_direction_counts[class_name][Direction.OUT.value] += 1\n",
    "\n",
    "            self.counted_ids.add(track_id)\n",
    "\n",
    "            if self.send_api_events:\n",
    "                report_hall_event(direction.value, self.hall_id)\n",
    "\n",
    "            if self.csv_logger:\n",
    "                self.csv_logger.add_row(\n",
    "                    {\n",
    "                        \"ID\": self.csv_row_id,\n",
    "                        \"ClassName\": class_name,\n",
    "                        \"Direction\": direction.value,\n",
    "                        \"Timestamp\": cv2.getTickCount(),\n",
    "                    }\n",
    "                )\n",
    "                self.csv_row_id += 1\n",
    "\n",
    "    def _cleanup_inactive_tracks(self, current_track_ids: List[int]) -> None:\n",
    "        \"\"\"Removes tracking data for objects no longer detected.\n",
    "\n",
    "        Args:\n",
    "            current_track_ids: List of currently active track IDs\n",
    "        \"\"\"\n",
    "        active_ids = set(current_track_ids)\n",
    "        inactive_ids = set(self.object_histories.keys()) - active_ids\n",
    "\n",
    "        for inactive_id in inactive_ids:\n",
    "            del self.object_histories[inactive_id]\n",
    "\n",
    "    def _reset_object_counted_status(\n",
    "        self, track_id, current_pos: Tuple[int, int], prev_pos: Tuple[int, int]\n",
    "    ) -> None:\n",
    "        \"\"\"Resets counting status for objects that have moved away from region.\n",
    "\n",
    "        Args:\n",
    "            track_id: ID of object to check\n",
    "            current_pos: Current object position\n",
    "            prev_pos: Previous object position\n",
    "        \"\"\"\n",
    "        if track_id in self.counted_ids:\n",
    "            if not self._is_crossing_region(current_pos, prev_pos):\n",
    "                self.counted_ids.remove(track_id)\n",
    "\n",
    "    def display_counts(self, image: np.ndarray) -> None:\n",
    "        \"\"\"Renders counting statistics on the output frame.\n",
    "\n",
    "        Args:\n",
    "            image: The frame to draw count information on\n",
    "        \"\"\"\n",
    "        text_y_position = 30\n",
    "        total_text = f\"Total: (IN {self.in_count_total} | OUT {self.out_count_total})\"\n",
    "        put_text_rect(image, total_text, (10, text_y_position), 0.7, 2)\n",
    "        text_y_position += 40\n",
    "\n",
    "        for class_name, counts in self.class_direction_counts.items():\n",
    "            count_in = counts[Direction.IN.value] > 0\n",
    "            count_out = counts[Direction.OUT.value] > 0\n",
    "\n",
    "            if count_in or count_out:\n",
    "                text_parts = [f\"{class_name.capitalize()}:\"]\n",
    "                if self.show_in_count and count_in:\n",
    "                    text_parts.append(f\"IN {counts[Direction.IN.value]}\")\n",
    "                if self.show_out_count and count_out:\n",
    "                    text_parts.append(f\"OUT {counts[Direction.OUT.value]}\")\n",
    "\n",
    "                label_text = \" \".join(text_parts)\n",
    "                put_text_rect(image, label_text, (10, text_y_position), 0.7, 1)\n",
    "                text_y_position += 30\n",
    "\n",
    "    def process_frame(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        boxes: List[List[int]],\n",
    "        track_ids: List[int],\n",
    "        class_ids: List[int],\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Processes a single video frame for object counting.\n",
    "\n",
    "        Args:\n",
    "            image: Input video frame\n",
    "            boxes: List of bounding boxes [x1,y1,x2,y2] for detected objects\n",
    "            track_ids: List of unique IDs for each detected object\n",
    "            class_ids: List of class IDs for each detected object\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Processed frame with visualizations\n",
    "        \"\"\"\n",
    "        self.draw_region(image)\n",
    "\n",
    "        for box, track_id, class_id in zip(boxes, track_ids, class_ids):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            centroid = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "            if self.draw_boxes:\n",
    "                label = self.class_names[class_id] if self.show_labels else None\n",
    "                draw_box_label(image, box, label)\n",
    "\n",
    "            # Store tracking history\n",
    "            history = self.object_histories[track_id]\n",
    "            history.append(centroid)\n",
    "            if len(history) > self.track_history_len:\n",
    "                history.pop(0)\n",
    "\n",
    "            prev_pos = (\n",
    "                self.object_histories[track_id][-2]\n",
    "                if len(self.object_histories[track_id]) > 1\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            self._process_object_count(centroid, prev_pos, track_id, class_id)\n",
    "\n",
    "            if self.draw_tracking:\n",
    "                draw_centroid_and_tracks(image, history)\n",
    "\n",
    "            self._reset_object_counted_status(track_id, centroid, prev_pos)\n",
    "\n",
    "        self._cleanup_inactive_tracks(track_ids)\n",
    "        self.display_counts(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def get_counts(self) -> Dict:\n",
    "        \"\"\"Retrieves current counting statistics.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - total_in: Total IN counts\n",
    "                - total_out: Total OUT counts\n",
    "                - classwise_counts: Per-class counting statistics\n",
    "                - active_objects: Number of currently tracked objects\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_in\": self.in_count_total,\n",
    "            \"total_out\": self.out_count_total,\n",
    "            \"classwise_counts\": dict(self.class_direction_counts),\n",
    "            \"active_objects\": len(self.object_histories),\n",
    "        }\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all counting statistics and tracking data.\"\"\"\n",
    "        self.in_count_total = 0\n",
    "        self.out_count_total = 0\n",
    "        self.counted_ids.clear()\n",
    "        self.class_direction_counts.clear()\n",
    "        self.object_histories.clear()\n",
    "        self.csv_row_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554bfeb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3eb3c5",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5a898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ObjectCounter'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'region': array([[  25,  590],\n",
       "        [1260,  590]]),\n",
       " 'class_names': {0: 'pedestrian',\n",
       "  1: 'people',\n",
       "  2: 'bicycle',\n",
       "  3: 'car',\n",
       "  4: 'van',\n",
       "  5: 'truck',\n",
       "  6: 'tricycle',\n",
       "  7: 'awning-tricycle',\n",
       "  8: 'bus',\n",
       "  9: 'motor'},\n",
       " 'csv_path': './data/outputs/record.csv',\n",
       " 'draw_tracking': True,\n",
       " 'draw_boxes': True,\n",
       " 'show_labels': False,\n",
       " 'region_color': (200, 200, 0),\n",
       " 'line_thickness': 2,\n",
       " 'show_in_count': True,\n",
       " 'show_out_count': True,\n",
       " 'track_history_len': 25,\n",
       " 'crossing_threshold': 0.5,\n",
       " 'send_api_events': False,\n",
       " 'hall_id': 1,\n",
       " 'region_info': RegionInfo(region_type=<RegionType.LINE: 'LINE'>, geometry=<LINESTRING (25 590, 1260 590)>, orientation=<Orientation.HORIZONTAL: 'HORIZONTAL'>)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x960 57 pedestrians, 409.3ms\n",
      "Speed: 11.1ms preprocess, 409.3ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 56 pedestrians, 450.8ms\n",
      "Speed: 11.2ms preprocess, 450.8ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 364.8ms\n",
      "Speed: 11.8ms preprocess, 364.8ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 56 pedestrians, 324.1ms\n",
      "Speed: 11.3ms preprocess, 324.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 332.0ms\n",
      "Speed: 10.9ms preprocess, 332.0ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 54 pedestrians, 362.0ms\n",
      "Speed: 11.8ms preprocess, 362.0ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 330.6ms\n",
      "Speed: 11.8ms preprocess, 330.6ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 328.3ms\n",
      "Speed: 11.1ms preprocess, 328.3ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 56 pedestrians, 380.6ms\n",
      "Speed: 11.4ms preprocess, 380.6ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 54 pedestrians, 397.0ms\n",
      "Speed: 12.0ms preprocess, 397.0ms inference, 3.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 394.0ms\n",
      "Speed: 14.4ms preprocess, 394.0ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 393.9ms\n",
      "Speed: 11.5ms preprocess, 393.9ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 53 pedestrians, 372.0ms\n",
      "Speed: 12.4ms preprocess, 372.0ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 54 pedestrians, 380.1ms\n",
      "Speed: 11.7ms preprocess, 380.1ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 58 pedestrians, 329.1ms\n",
      "Speed: 12.4ms preprocess, 329.1ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 350.7ms\n",
      "Speed: 12.9ms preprocess, 350.7ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 394.6ms\n",
      "Speed: 12.4ms preprocess, 394.6ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 325.8ms\n",
      "Speed: 11.8ms preprocess, 325.8ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 320.4ms\n",
      "Speed: 11.8ms preprocess, 320.4ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 348.1ms\n",
      "Speed: 13.7ms preprocess, 348.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 347.9ms\n",
      "Speed: 14.8ms preprocess, 347.9ms inference, 3.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 361.1ms\n",
      "Speed: 13.9ms preprocess, 361.1ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 327.6ms\n",
      "Speed: 11.3ms preprocess, 327.6ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 325.5ms\n",
      "Speed: 11.8ms preprocess, 325.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 365.5ms\n",
      "Speed: 11.3ms preprocess, 365.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 337.1ms\n",
      "Speed: 12.2ms preprocess, 337.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 317.4ms\n",
      "Speed: 10.9ms preprocess, 317.4ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 370.4ms\n",
      "Speed: 11.4ms preprocess, 370.4ms inference, 3.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 67 pedestrians, 322.8ms\n",
      "Speed: 11.3ms preprocess, 322.8ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 352.4ms\n",
      "Speed: 10.9ms preprocess, 352.4ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 67 pedestrians, 316.8ms\n",
      "Speed: 11.0ms preprocess, 316.8ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 335.8ms\n",
      "Speed: 11.1ms preprocess, 335.8ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 362.9ms\n",
      "Speed: 11.3ms preprocess, 362.9ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 330.0ms\n",
      "Speed: 11.0ms preprocess, 330.0ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 317.4ms\n",
      "Speed: 10.9ms preprocess, 317.4ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 333.9ms\n",
      "Speed: 11.3ms preprocess, 333.9ms inference, 2.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 321.8ms\n",
      "Speed: 11.2ms preprocess, 321.8ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 328.9ms\n",
      "Speed: 10.5ms preprocess, 328.9ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 323.3ms\n",
      "Speed: 11.3ms preprocess, 323.3ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 314.4ms\n",
      "Speed: 10.8ms preprocess, 314.4ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 67 pedestrians, 346.4ms\n",
      "Speed: 10.9ms preprocess, 346.4ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 371.5ms\n",
      "Speed: 11.5ms preprocess, 371.5ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 67 pedestrians, 321.0ms\n",
      "Speed: 11.2ms preprocess, 321.0ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 374.1ms\n",
      "Speed: 12.0ms preprocess, 374.1ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 329.0ms\n",
      "Speed: 11.6ms preprocess, 329.0ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 365.7ms\n",
      "Speed: 10.6ms preprocess, 365.7ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 68 pedestrians, 350.6ms\n",
      "Speed: 11.5ms preprocess, 350.6ms inference, 2.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 68 pedestrians, 321.1ms\n",
      "Speed: 11.1ms preprocess, 321.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 68 pedestrians, 352.5ms\n",
      "Speed: 12.6ms preprocess, 352.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 68 pedestrians, 318.4ms\n",
      "Speed: 11.3ms preprocess, 318.4ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 330.4ms\n",
      "Speed: 11.2ms preprocess, 330.4ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 343.4ms\n",
      "Speed: 11.1ms preprocess, 343.4ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 319.4ms\n",
      "Speed: 11.5ms preprocess, 319.4ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 322.0ms\n",
      "Speed: 10.7ms preprocess, 322.0ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 318.6ms\n",
      "Speed: 10.9ms preprocess, 318.6ms inference, 3.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 317.7ms\n",
      "Speed: 11.3ms preprocess, 317.7ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 340.5ms\n",
      "Speed: 10.8ms preprocess, 340.5ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 324.4ms\n",
      "Speed: 10.8ms preprocess, 324.4ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 325.9ms\n",
      "Speed: 11.1ms preprocess, 325.9ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 349.9ms\n",
      "Speed: 11.4ms preprocess, 349.9ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 315.4ms\n",
      "Speed: 11.0ms preprocess, 315.4ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 66 pedestrians, 317.7ms\n",
      "Speed: 11.1ms preprocess, 317.7ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 330.9ms\n",
      "Speed: 11.4ms preprocess, 330.9ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 389.7ms\n",
      "Speed: 11.6ms preprocess, 389.7ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 357.0ms\n",
      "Speed: 11.2ms preprocess, 357.0ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 313.7ms\n",
      "Speed: 11.2ms preprocess, 313.7ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 328.6ms\n",
      "Speed: 10.7ms preprocess, 328.6ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 344.4ms\n",
      "Speed: 11.2ms preprocess, 344.4ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 316.9ms\n",
      "Speed: 11.2ms preprocess, 316.9ms inference, 6.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 320.5ms\n",
      "Speed: 11.3ms preprocess, 320.5ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 345.8ms\n",
      "Speed: 11.5ms preprocess, 345.8ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 328.5ms\n",
      "Speed: 10.8ms preprocess, 328.5ms inference, 2.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 324.6ms\n",
      "Speed: 11.0ms preprocess, 324.6ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 330.2ms\n",
      "Speed: 11.6ms preprocess, 330.2ms inference, 2.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 318.1ms\n",
      "Speed: 11.5ms preprocess, 318.1ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 341.5ms\n",
      "Speed: 10.3ms preprocess, 341.5ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 314.8ms\n",
      "Speed: 10.5ms preprocess, 314.8ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 313.3ms\n",
      "Speed: 11.0ms preprocess, 313.3ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 371.3ms\n",
      "Speed: 11.1ms preprocess, 371.3ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 317.0ms\n",
      "Speed: 11.0ms preprocess, 317.0ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 322.0ms\n",
      "Speed: 11.0ms preprocess, 322.0ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 334.5ms\n",
      "Speed: 11.1ms preprocess, 334.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 311.6ms\n",
      "Speed: 11.3ms preprocess, 311.6ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 319.0ms\n",
      "Speed: 10.7ms preprocess, 319.0ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 397.1ms\n",
      "Speed: 11.8ms preprocess, 397.1ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 330.5ms\n",
      "Speed: 11.3ms preprocess, 330.5ms inference, 2.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 341.9ms\n",
      "Speed: 11.0ms preprocess, 341.9ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 321.8ms\n",
      "Speed: 10.6ms preprocess, 321.8ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 312.1ms\n",
      "Speed: 10.7ms preprocess, 312.1ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 346.2ms\n",
      "Speed: 11.4ms preprocess, 346.2ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 314.8ms\n",
      "Speed: 11.3ms preprocess, 314.8ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 349.4ms\n",
      "Speed: 11.4ms preprocess, 349.4ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 382.7ms\n",
      "Speed: 11.1ms preprocess, 382.7ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 336.4ms\n",
      "Speed: 11.3ms preprocess, 336.4ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 376.9ms\n",
      "Speed: 11.2ms preprocess, 376.9ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 314.7ms\n",
      "Speed: 11.2ms preprocess, 314.7ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 321.7ms\n",
      "Speed: 11.8ms preprocess, 321.7ms inference, 5.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 345.8ms\n",
      "Speed: 11.6ms preprocess, 345.8ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 314.9ms\n",
      "Speed: 10.7ms preprocess, 314.9ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 323.1ms\n",
      "Speed: 10.7ms preprocess, 323.1ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 338.8ms\n",
      "Speed: 11.0ms preprocess, 338.8ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 316.9ms\n",
      "Speed: 11.7ms preprocess, 316.9ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 316.6ms\n",
      "Speed: 11.4ms preprocess, 316.6ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 326.0ms\n",
      "Speed: 11.0ms preprocess, 326.0ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 308.2ms\n",
      "Speed: 10.7ms preprocess, 308.2ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 342.0ms\n",
      "Speed: 10.5ms preprocess, 342.0ms inference, 3.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 328.3ms\n",
      "Speed: 14.1ms preprocess, 328.3ms inference, 3.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 307.1ms\n",
      "Speed: 11.3ms preprocess, 307.1ms inference, 5.6ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 348.1ms\n",
      "Speed: 11.4ms preprocess, 348.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 307.7ms\n",
      "Speed: 11.1ms preprocess, 307.7ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 330.7ms\n",
      "Speed: 11.2ms preprocess, 330.7ms inference, 3.3ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 321.1ms\n",
      "Speed: 11.2ms preprocess, 321.1ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 322.4ms\n",
      "Speed: 11.0ms preprocess, 322.4ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 346.6ms\n",
      "Speed: 11.9ms preprocess, 346.6ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 65 pedestrians, 317.3ms\n",
      "Speed: 10.6ms preprocess, 317.3ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 324.6ms\n",
      "Speed: 13.7ms preprocess, 324.6ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 345.8ms\n",
      "Speed: 10.3ms preprocess, 345.8ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 320.2ms\n",
      "Speed: 10.7ms preprocess, 320.2ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 356.9ms\n",
      "Speed: 11.5ms preprocess, 356.9ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 413.4ms\n",
      "Speed: 12.2ms preprocess, 413.4ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 317.0ms\n",
      "Speed: 10.9ms preprocess, 317.0ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 64 pedestrians, 380.5ms\n",
      "Speed: 11.3ms preprocess, 380.5ms inference, 3.4ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 63 pedestrians, 405.6ms\n",
      "Speed: 11.3ms preprocess, 405.6ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 311.7ms\n",
      "Speed: 11.5ms preprocess, 311.7ms inference, 3.5ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 332.4ms\n",
      "Speed: 10.4ms preprocess, 332.4ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 307.9ms\n",
      "Speed: 10.8ms preprocess, 307.9ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 321.5ms\n",
      "Speed: 11.1ms preprocess, 321.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 353.0ms\n",
      "Speed: 10.8ms preprocess, 353.0ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 394.8ms\n",
      "Speed: 10.6ms preprocess, 394.8ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 316.5ms\n",
      "Speed: 11.4ms preprocess, 316.5ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 62 pedestrians, 342.2ms\n",
      "Speed: 10.7ms preprocess, 342.2ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 315.0ms\n",
      "Speed: 12.0ms preprocess, 315.0ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 314.8ms\n",
      "Speed: 10.9ms preprocess, 314.8ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 58 pedestrians, 331.1ms\n",
      "Speed: 11.5ms preprocess, 331.1ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 327.6ms\n",
      "Speed: 10.4ms preprocess, 327.6ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 56 pedestrians, 350.2ms\n",
      "Speed: 11.2ms preprocess, 350.2ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 316.9ms\n",
      "Speed: 10.5ms preprocess, 316.9ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 58 pedestrians, 329.0ms\n",
      "Speed: 10.3ms preprocess, 329.0ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 343.1ms\n",
      "Speed: 10.7ms preprocess, 343.1ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 57 pedestrians, 313.0ms\n",
      "Speed: 10.7ms preprocess, 313.0ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 56 pedestrians, 326.5ms\n",
      "Speed: 10.6ms preprocess, 326.5ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 341.9ms\n",
      "Speed: 11.5ms preprocess, 341.9ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 326.8ms\n",
      "Speed: 11.4ms preprocess, 326.8ms inference, 2.9ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 379.0ms\n",
      "Speed: 10.9ms preprocess, 379.0ms inference, 3.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 373.1ms\n",
      "Speed: 11.4ms preprocess, 373.1ms inference, 3.2ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 362.4ms\n",
      "Speed: 11.3ms preprocess, 362.4ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 358.5ms\n",
      "Speed: 11.3ms preprocess, 358.5ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 308.0ms\n",
      "Speed: 10.9ms preprocess, 308.0ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 314.6ms\n",
      "Speed: 13.4ms preprocess, 314.6ms inference, 2.7ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 392.3ms\n",
      "Speed: 11.6ms preprocess, 392.3ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 324.5ms\n",
      "Speed: 10.6ms preprocess, 324.5ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 61 pedestrians, 317.6ms\n",
      "Speed: 10.7ms preprocess, 317.6ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 351.7ms\n",
      "Speed: 11.0ms preprocess, 351.7ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 58 pedestrians, 349.7ms\n",
      "Speed: 11.1ms preprocess, 349.7ms inference, 3.1ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 345.7ms\n",
      "Speed: 11.3ms preprocess, 345.7ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 60 pedestrians, 363.7ms\n",
      "Speed: 10.7ms preprocess, 363.7ms inference, 3.0ms postprocess per image at shape (1, 3, 544, 960)\n",
      "\n",
      "0: 544x960 59 pedestrians, 336.1ms\n",
      "Speed: 10.7ms preprocess, 336.1ms inference, 2.8ms postprocess per image at shape (1, 3, 544, 960)\n",
      "{'total_in': 16, 'total_out': 0, 'classwise_counts': {'pedestrian': {'IN': 16, 'OUT': 0}}, 'active_objects': 59}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "# from src.counting import ObjectCounter\n",
    "\n",
    "\n",
    "def process_video(\n",
    "    model_path: str,\n",
    "    csv_path: str,\n",
    "    input_video_path: str,\n",
    "    region: list,\n",
    "    draw_boxes: bool = True,\n",
    "    draw_tracking: bool = True,\n",
    "    show_labels: bool = True,\n",
    "    send_api_events: bool = True,\n",
    "    hall_id: int = 1,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # Create ObjectCounter instance\n",
    "    counter = ObjectCounter(\n",
    "        region=region,\n",
    "        class_names=model.names,\n",
    "        csv_path=csv_path,\n",
    "        draw_boxes=draw_boxes,\n",
    "        draw_tracking=draw_tracking,\n",
    "        show_labels=show_labels,\n",
    "        send_api_events=send_api_events,\n",
    "        hall_id=hall_id,\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Run YOLO model on the frame\n",
    "        results = model.track(frame, persist=True, verbose=verbose, classes=[0])\n",
    "\n",
    "        if results[0].boxes.id is None:\n",
    "            continue\n",
    "\n",
    "        # Extract bounding boxes, track IDs, and class indices\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy().astype(int).tolist()\n",
    "        track_ids = results[0].boxes.id.cpu().numpy().astype(int).tolist()\n",
    "        clss = results[0].boxes.cls.cpu().numpy().astype(int).tolist()\n",
    "\n",
    "        processed_frame = counter.process_frame(frame, boxes, track_ids, clss)\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow(\"Processed Frame\", processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(counter.get_counts())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"./models/VisDrone_YOLOv8s_Best.pt\"\n",
    "    csv_path = \"./data/outputs/record.csv\"\n",
    "    input_video_path = \"./data/inputs/people_in_marathon.mp4\"\n",
    "\n",
    "    # Define the counting region (line or polygon)\n",
    "    # region = [(25, 470), (25, 500), (1260, 500), (1260, 470)]\n",
    "    # region = [(25, 500), (1260, 500)]\n",
    "\n",
    "    region = [(25, 590), (1260, 590)]\n",
    "\n",
    "    process_video(\n",
    "        model_path=model_path,\n",
    "        csv_path=csv_path,\n",
    "        input_video_path=input_video_path,\n",
    "        region=region,\n",
    "        draw_boxes=True,\n",
    "        draw_tracking=True,\n",
    "        show_labels=False,\n",
    "        send_api_events=False,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e814cf7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c05ad9",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ“Œ Notes:\n",
    "- This project is part of a university system aimed at improving student attendance automation and campus monitoring.\n",
    "- Developed as a component of a larger Smart Education Infrastructure.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
